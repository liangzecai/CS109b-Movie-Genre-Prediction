{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS109b Final Project\n",
    "\n",
    "# Milestone 3\n",
    "\n",
    "\n",
    "by Danqing Wang, Wenshan Zheng, Zecai Liang\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1. Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable Group 1: ['year', 'rating', 'votes', 'popularity_TMDB', 'runtime_TMDB']\n",
    "- variable ['year', 'rating', 'votes'] have missing values as 'NaN': we impute the missing value in training and test data with the mean value of training data\n",
    "- [popularity_TMDB', 'runtime_TMDB'] have missing values as 0: we impute the missing value in training and test data with the mean value of training data\n",
    "\n",
    "### Variables Group 2: ['title', 'plot', 'plot outline', 'overview_TMDB', 'tagline_TMDB']\n",
    "- we combine all the text information for one movie, split the paragraph of string into bag-of-words, and return the top 10 PCs\n",
    "- we used the PCA model trained from the training data to transform the test data\n",
    "\n",
    "### Variable Group 3: ['mpaa']\n",
    "- we extracted the text as the reason for 'mpaa', and applied similar text analysis as variable group 2, and return return the top 10 PCs\n",
    "\n",
    "### Variable Group 4: ['director', 'cast', 'production company', 'writer']\n",
    "- @ Danqing\n",
    "\n",
    "### Variables Group 5: ['animation department', 'original music']\n",
    "- we extract the number of person ID for both department, representing the number of staff working there"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Responses\n",
    "@ Wehshan\n",
    "- choose one between 'music' and 'musical'\n",
    "- delete 'TY-movie'\n",
    "- combined some genres (details and reason)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_feature = pd.read_csv(\"feature_final.csv\", index_col = 0)\n",
    "df_genre = pd.read_csv(\"genre_final.csv\", index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# x_train, x_test, y_train, y_test = generate_train_test_data(feature_data = df_feature, genre_data = df_genre, \n",
    "                                                             n_sample = 5000, train_ratio = 0.5, ran_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# x_train.to_csv(\"x_train.csv\")\n",
    "# x_test.to_csv(\"x_test.csv\")\n",
    "# y_train.to_csv(\"y_train.csv\")\n",
    "# y_test.to_csv(\"y_test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run functions in the supplement before running this function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###### Function to generate train and test data for classification models #######\n",
    "### ------ Input ---------- ###\n",
    "# feature_data: a data frame with original features from IMDB and TMDB\n",
    "# genre_data: a data grame with genres (already merged turned into one-hot coding), \n",
    "#             matching the feature_data by 'imdb_ids'\n",
    "# n_sample: the number of data points to sample from the input data frame to fit model\n",
    "# train_ratio: the percentage of training data among sampled data\n",
    "# ran_state: seed for random sampling\n",
    "\n",
    "\n",
    "def generate_train_test_data(feature_data, genre_data, n_sample = 5000, train_ratio = 0.5, ran_state = 0):\n",
    "    \n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import re\n",
    "    import random\n",
    "    import warnings\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "    # ------------------------------------------------------------------------------------------------------ #     \n",
    "\n",
    "    ### sample data ###\n",
    "    \n",
    "    data_x = feature_data.sample(n = n_sample, random_state = ran_state)\n",
    "    data_y = genre_data.ix[data_x.index.values,]\n",
    "    \n",
    "    ### split into train and test data ###\n",
    "    k = int(n_sample * train_ratio)\n",
    "    \n",
    "    x_train = data_x[:k]\n",
    "    x_test = data_x[k:]\n",
    "    y_train = data_y[:k]\n",
    "    y_test = data_y[k:]\n",
    "    \n",
    "    \n",
    "    # ------------------------------------------------------------------------------------------------------ # \n",
    "    ### Variable Group 1: ['year', 'rating', 'votes', 'popularity_TMDB', 'runtime_TMDB'] ###\n",
    "\n",
    "    val_group1 = ['year', 'rating', 'votes', 'popularity_TMDB', 'runtime_TMDB']\n",
    "    \n",
    "    ## missing value imputation ##\n",
    "    # use the mean from train data to fill test data \n",
    "\n",
    "    # 'year', 'rating', 'votes' have missing values as 'NaN', fill by column mean\n",
    "    x_train.ix[x_train['year'].isnull(), 'year'] = x_train['year'].mean()\n",
    "    x_test.ix[x_test['year'].isnull(), 'year'] = x_train['year'].mean()\n",
    "\n",
    "    x_train.ix[x_train['rating'].isnull(), 'rating'] = x_train['rating'].mean()\n",
    "    x_test.ix[x_test['rating'].isnull(), 'rating'] = x_train['rating'].mean()\n",
    "\n",
    "    x_train.ix[x_train['votes'].isnull(), 'votes'] = x_train['votes'].mean()\n",
    "    x_test.ix[x_test['votes'].isnull(), 'votes'] = x_train['votes'].mean()\n",
    "\n",
    "    # 'popularity_TMDB', 'runtime_TMDB' have missing values as 0, replace by column mean\n",
    "    x_train.ix[x_train['popularity_TMDB'] == 0, 'popularity_TMDB'] = x_train['popularity_TMDB'].mean()\n",
    "    x_test.ix[x_test['runtime_TMDB'] == 0, 'runtime_TMDB'] = x_train['runtime_TMDB'].mean() \n",
    "    \n",
    "    ## use x_train_new, x_test_new to record the transformed data\n",
    "    x_train_group1 = x_train[val_group1]\n",
    "    x_test_group1 = x_test[val_group1]\n",
    "\n",
    "    \n",
    "    # ------------------------------------------------------------------------------------------------------ # \n",
    "    ### Variables Group 2: ['title', 'plot', 'plot outline', 'overview_TMDB', 'tagline_TMDB', 'mpaa_reason'] ###\n",
    "\n",
    "    ## combine the text in ['title', 'plot', 'plot outline', 'overview_TMDB', 'tagline_TMDB']\n",
    "\n",
    "    x_train_text = x_train['title'].str.cat([x_train['plot'], x_train['plot outline'], \n",
    "                              x_train['overview_TMDB'], x_train['tagline_TMDB']], \n",
    "                              na_rep = \" \")\n",
    "    x_train_text = pd.DataFrame(x_train_text, index = x_train.index)\n",
    "\n",
    "    x_test_text = x_test['title'].str.cat([x_test['plot'], x_train['plot outline'], \n",
    "                              x_test['overview_TMDB'], x_test['tagline_TMDB']], \n",
    "                              na_rep = \" \")\n",
    "    x_test_text = pd.DataFrame(x_test_text, index = x_test.index)\n",
    "\n",
    "    ## apply text analysis on combined text and return the top 10 PCs\n",
    "    x_train_group2, x_test_group2 = text_analysis(x_train_text, x_test_text,\n",
    "                                             val_name = 'text', n_components = 10)\n",
    "\n",
    "    \n",
    "    # ------------------------------------------------------------------------------------------------------ # \n",
    "    ### Variables Group 3: [mpaa_reason'] ###\n",
    "\n",
    "    ## apply text analysis on 'mpaa_reason' and return thr PCs that cover 60% variance\n",
    "    x_train_mpaa = pd.DataFrame(x_train['mpaa_reason'], index = x_train.index)\n",
    "    x_test_mpaa = pd.DataFrame(x_test['mpaa_reason'], index = x_test.index)\n",
    "    \n",
    "    x_train_group3, x_test_group3 = text_analysis(x_train_mpaa, x_test_mpaa,\n",
    "                                             val_name = 'mpaa', n_components = 10)\n",
    "    \n",
    "    \n",
    "    # ------------------------------------------------------------------------------------------------------ # \n",
    "    ### Variables Group 4: ['director', 'cast', 'production company', 'writer'] ###\n",
    "    \n",
    "    x_train_group4_1, x_test_group4_1 = top_features(train_feature = x_train, train_genre = y_train,\n",
    "                                        test_feature = x_test, test_genre = y_test,\n",
    "                                        val_name = 'director',\n",
    "                                        val_n = 1)\n",
    "    \n",
    "    x_train_group4_2, x_test_group4_2 = top_features(train_feature = x_train, train_genre = y_train,\n",
    "                                        test_feature = x_test, test_genre = y_test,\n",
    "                                        val_name = 'writer',\n",
    "                                        val_n = 1)\n",
    "\n",
    "    x_train_group4_3, x_test_group4_3 = top_features_pca(train_feature = x_train, train_genre = y_train,\n",
    "                                        test_feature = x_test, test_genre = y_test,\n",
    "                                         val_name = 'cast',\n",
    "                                         val_n = 5,\n",
    "                                         pca_n = 10)\n",
    "\n",
    "    x_train_group4_4, x_test_group4_4 = top_features_pca(train_feature = x_train, train_genre = y_train,\n",
    "                                        test_feature = x_test, test_genre = y_test,\n",
    "                                         val_name = 'production companies',\n",
    "                                         val_n = 5,\n",
    "                                         pca_n = 5)\n",
    "     \n",
    "    x_train_group4 = pd.concat([x_train_group4_1, x_train_group4_2,\n",
    "                               x_train_group4_3, x_train_group4_4], axis = 1)\n",
    "    x_test_group4 = pd.concat([x_test_group4_1, x_test_group4_2,\n",
    "                               x_test_group4_3, x_test_group4_4], axis = 1)\n",
    "    \n",
    "    # ------------------------------------------------------------------------------------------------------ # \n",
    "    ### Variables Group 5: ['animation department', 'original music'] ###\n",
    "    \n",
    "    # return x_train_group5, x_test_group5\n",
    "    \n",
    "    x_train_group5_1, x_test_group5_1 = feature_to_count(train_feature = x_train, train_genre = y_train,\n",
    "                                        test_feature = x_test, test_genre = y_test,\n",
    "                                        val_name = 'animation department')\n",
    "\n",
    "    x_train_group5_2, x_test_group5_2 = feature_to_count(train_feature = x_train, train_genre = y_train,\n",
    "                                        test_feature = x_test, test_genre = y_test,\n",
    "                                        val_name = 'original music')\n",
    "    \n",
    "    \n",
    "    x_train_group5 = pd.concat([x_train_group5_1, x_train_group5_2], axis = 1)\n",
    "    x_test_group5 = pd.concat([x_test_group5_1, x_test_group5_2], axis = 1)\n",
    "    \n",
    "    # ------------------------------------------------------------------------------------------------------ # \n",
    "    ### Combine Engineered Features ###\n",
    "    x_train_new = pd.concat([x_train_group1, x_train_group2, x_train_group3, \n",
    "                             x_train_group4, x_train_group5], axis=1)\n",
    "    \n",
    "    x_test_new = pd.concat([x_test_group1, x_test_group2, x_test_group3, \n",
    "                             x_test_group4, x_test_group5], axis=1)\n",
    "    \n",
    "    return (x_train_new, x_test_new, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supplement: functions that are used in the above meta-function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### part of the function to apply text analysis to a data series #####\n",
    "#### to transform a text paragraph to bag-of-words than to one-hot coding\n",
    "\n",
    "\n",
    "#----------------------------------------------------------------------------------------------------\n",
    "### Input ###\n",
    "        # data: a series for text analysis\n",
    "\n",
    "        \n",
    "### Output ###\n",
    "        # the transformed data in one-hot coding\n",
    "#----------------------------------------------------------------------------------------------------        \n",
    "\n",
    "\n",
    "\n",
    "def text_to_matrix(data):\n",
    "    \n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import re\n",
    "    import warnings\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    ## --------------- Bag-of-Words --------------- ##\n",
    "    \n",
    "    ## string to list\n",
    "    import re\n",
    "    col_words = []\n",
    "    \n",
    "    for i in range(data.shape[0]):\n",
    "    \n",
    "        if type(data.iloc[i,0]) == str: \n",
    "            letters_only = re.sub(\"[^a-zA-Z]\", \" \" , data.iloc[i,0]) # remove non-letter\n",
    "            lower_case = letters_only.lower().split()   # Convert to lower case # Split into words\n",
    "            \n",
    "            # avoid downloading nltk\n",
    "            # from NLTK stopwords https://pythonprogramming.net/stop-words-nltk-tutorial/\n",
    "            stops = {'ourselves', 'hers', 'between', 'yourself', 'but', 'again', 'there', 'about', 'once', \n",
    "                     'during', 'out', 'very', 'having', 'with', 'they', 'own', 'an', 'be', 'some', 'for', \n",
    "                     'do', 'its', 'yours', 'such', 'into', 'of', 'most', 'itself', 'other', 'off', 'is', \n",
    "                     's', 'am', 'or', 'who', 'as', 'from', 'him', 'each', 'the', 'themselves', 'until', \n",
    "                     'below', 'are', 'we', 'these', 'your', 'his', 'through', 'don', 'nor', 'me', 'were', \n",
    "                     'her', 'more', 'himself', 'this', 'down', 'should', 'our', 'their', 'while', 'above', \n",
    "                     'both', 'up', 'to', 'ours', 'had', 'she', 'all', 'no', 'when', 'at', 'any', 'before', \n",
    "                     'them', 'same', 'and', 'been', 'have', 'in', 'will', 'on', 'does', 'yourselves', 'then', \n",
    "                     'that', 'because', 'what', 'over', 'why', 'so', 'can', 'did', 'not', 'now', 'under', 'he', \n",
    "                     'you', 'herself', 'has', 'just', 'where', 'too', 'only', 'myself', 'which', 'those', 'i', \n",
    "                     'after', 'few', 'whom', 't', 'being', 'if', 'theirs', 'my', 'against', 'a', 'by', 'doing', \n",
    "                     'it', 'how', 'further', 'was', 'here', 'than'} \n",
    "            meaningful_words = [w for w in lower_case if not w in stops]  # Remove stop words from \"words\"\n",
    "            \n",
    "            words = ( \" \".join(meaningful_words))\n",
    "    \n",
    "        else: words = \"NA\"\n",
    "       \n",
    "        col_words.append(words)\n",
    "        \n",
    "        \n",
    "    \n",
    "    ## list to vector\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "    # Initialize the \"CountVectorizer\" object\n",
    "    vectorizer = CountVectorizer(analyzer = \"word\",   \n",
    "                                 tokenizer = None,    \n",
    "                                 preprocessor = None, \n",
    "                                 stop_words = None,   \n",
    "                                 max_features = 10000)\n",
    "\n",
    "    data_array = vectorizer.fit_transform(col_words)\n",
    "    data_array = pd.DataFrame(data_array.toarray())\n",
    "    data_array.columns = vectorizer.get_feature_names()\n",
    "    \n",
    "    return data_array\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for Variable ['title', 'plot', 'plot outline', 'overview_TMDB', 'tagline_TMDB', 'mpaa_reason']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### Function to apply text analysis to a column with `colname` in the data file `filename`\n",
    "\n",
    "\n",
    "#----------------------------------------------------------------------------------------------------\n",
    "### Input ###\n",
    "        # train: the train data in one-hot coding\n",
    "        # test: the test data in one-hot coding\n",
    "        # val_name:  variable name that's used in naming the columns as \"val_name_PCi\"\n",
    "        # n_components: if value is int, the number of PCs to return\n",
    "                       # if value between (0,1), the variance explained by the PCs returnd\n",
    "        \n",
    "### Output ###\n",
    "        # data matrix of engineered feature, one for train data and one for test data\n",
    "#----------------------------------------------------------------------------------------------------        \n",
    "\n",
    "\n",
    "def text_analysis(train, test, val_name, n_components):\n",
    "    \n",
    "    ## turn each text paragraph into one-hot coding\n",
    "    train_array = text_to_matrix(train)\n",
    "    test_array = text_to_matrix(test)\n",
    "    \n",
    "    ## take the union set of words in train and text data as column\n",
    "    ## words that don't show up are assigned 0\n",
    "    align_column = pd.concat([train_array,test_array], axis=0).fillna(0)\n",
    "    \n",
    "    ## split into train and text after aligning the columns\n",
    "    train_array = align_column.iloc[0:train_array.shape[0], ]\n",
    "    test_array = align_column.iloc[train_array.shape[0]:, ]\n",
    "    \n",
    "    ## PCA\n",
    "    from sklearn.decomposition import PCA\n",
    "    pca = PCA(n_components = n_components)\n",
    "    pca.fit(train_array)\n",
    "    train_new = pd.DataFrame(pca.transform(train_array), index = train.index)\n",
    "    test_new = pd.DataFrame(pca.transform(test_array), index = test.index)\n",
    "    \n",
    "    col_names = []\n",
    "    for i in range(train_new.shape[1]):\n",
    "        i_name = val_name + \"_PC\" + str(i+1)\n",
    "        col_names.append(i_name)\n",
    "        \n",
    "    train_new.columns = col_names\n",
    "    test_new.columns = col_names\n",
    "    \n",
    "    return (train_new, test_new)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function used in the following functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Function to convert to string list [u'Action', u'Adventure', u'Fantasy'] into dummy coding\n",
    "## input: \n",
    "           # data = orignal data frame, \n",
    "           # val_name = name of the variable\n",
    "## output: a data frame\n",
    "\n",
    "def string_to_vector(data, val_name):\n",
    "    \n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    \n",
    "    # convert any np.nan to a string 'nan'\n",
    "    data[val_name][pd.isnull(data[val_name])] = 'nan'\n",
    "    \n",
    "    vectorizer = CountVectorizer(analyzer = \"word\",   \n",
    "                                             tokenizer = None,    \n",
    "                                             preprocessor = None, \n",
    "                                             stop_words = None,   \n",
    "                                             max_features = 50000)\n",
    "\n",
    "    val_data = vectorizer.fit_transform(data[val_name])\n",
    "    df_val = pd.DataFrame(val_data.toarray())\n",
    "    df_val.columns = vectorizer.get_feature_names()\n",
    "    df_val.index = data.index\n",
    "    \n",
    "    return df_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for Variable ['director', 'writer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Function considers a particular feature of interest (e.g. director, writer) \n",
    "### and picks out the top val_n most important value in each genre\n",
    "### Input: \n",
    "# - train_feature: df with 2500 train observations, all feature columns, indexed with imdb_ids\n",
    "# - train_genre: df with 2500 train observations, all genre columns, indexed with imdb_ids\n",
    "# - test_feature: df with 2500 test observations, all feature columns, indexed with imdb_ids \n",
    "# - test_genre: df with 2500 test observations, all genre columns, indexed with imdb_ids\n",
    "# - val_name # feature of interest, eg. director\n",
    "# - val_n # number of top values to take, eg. 1\n",
    "### Output:\n",
    "# - train_val: df with 2500 train observations, new columns of top directors, indexed with imdb_ids\n",
    "# - test_val: df with 2500 test observations, new columns of top directors, indexed with imdb_ids\n",
    "\n",
    "def top_features(train_feature, train_genre, test_feature, test_genre,\n",
    "                val_name = 'director', val_n = 1):\n",
    "\n",
    "    # convert feature of interest into dummy variables in train set and test set\n",
    "    feature_val_train = string_to_vector(train_feature, val_name)\n",
    "    feature_val_test = string_to_vector(test_feature, val_name)\n",
    "\n",
    "    # create a dataframe with columns consisting of all directors and all genres, rows are movie entries \n",
    "    feature_val_genre = pd.concat([feature_val_train, train_genre], axis = 1)\n",
    "    \n",
    "    # generate list of top directors in each genre \n",
    "    val_list = []\n",
    "    for i in train_genre.columns:\n",
    "        sum_val_in_genre = feature_val_genre.ix[feature_val_genre[i] == 1, range(feature_val_train.shape[1]-1)].sum(axis = 0)\n",
    "        sum_val_in_genre_sorted = sum_val_in_genre.sort(inplace=False, ascending = False)\n",
    "        for j in range(val_n):\n",
    "            top_val_in_genre = sum_val_in_genre_sorted.index[j]\n",
    "            val_list.append(top_val_in_genre)      \n",
    "\n",
    "    # output dataframes of movies with new columns\n",
    "    train_val = feature_val_train.ix[:, val_list] \n",
    "    test_val = feature_val_test.ix[:, val_list]\n",
    "    \n",
    "    # rename columns as director1, director2, etc\n",
    "    col_names = []\n",
    "    for i in range(train_val.shape[1]):\n",
    "            i_name = val_name + str(i)\n",
    "            col_names.append(i_name)\n",
    "    train_val.columns = col_names\n",
    "    test_val.columns = col_names\n",
    "\n",
    "    return(train_val, test_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for Variable ['cast','production company']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Function considers a particular feature of interest (e.g. cast, production companies) \n",
    "### and picks out the top val_n most important value in each genre\n",
    "### and then performs PCA, picks out the top pca_n numbers of components\n",
    "### Input: \n",
    "# - train_feature: df with 2500 train observations, all feature columns, indexed with imdb_ids\n",
    "# - train_genre: df with 2500 train observations, all genre columns, indexed with imdb_ids\n",
    "# - test_feature: df with 2500 test observations, all feature columns, indexed with imdb_ids \n",
    "# - test_genre: df with 2500 test observations, all genre columns, indexed with imdb_ids\n",
    "# - val_name # feature of interest, eg. director\n",
    "# - val_n # number of top values to take, eg. 5\n",
    "# - pca_n # number of pca components to retain, eg. 5\n",
    "### Output:\n",
    "# - train_val: df with 3000 train observations, columns as pca components, indexed with imdb_ids\n",
    "# - test_val: df with 2000 test observations, columns as pca components, indexed with imdb_ids\n",
    "\n",
    "def top_features_pca(train_feature, train_genre, test_feature, test_genre,\n",
    "                     val_name = 'cast',\n",
    "                     val_n = 5,\n",
    "                     pca_n = 5):\n",
    "    \n",
    "    ## Step 1, pick top casts in each genre using top_feature function \n",
    "    train_val, test_val = top_features(train_feature = train_feature,\n",
    "                    train_genre = train_genre,\n",
    "                    test_feature = test_feature,\n",
    "                    test_genre = test_genre,\n",
    "                    val_name = val_name,\n",
    "                    val_n = val_n)\n",
    "    \n",
    "    # replace any NA values in test set with 0 (or will have problem in pca.transform)\n",
    "    test_val = test_val.fillna(value=0)\n",
    "\n",
    "    ## Step 2, perform PCA\n",
    "    from sklearn.decomposition import PCA\n",
    "    \n",
    "    pca = PCA(n_components = pca_n, svd_solver = \"full\") # keep the first pca_n PCs\n",
    "    pca = pca.fit(train_val)\n",
    "    train_pca = pd.DataFrame(pca.transform(train_val), index = train_val.index)\n",
    "    test_pca = pd.DataFrame(pca.transform(test_val), index = test_val.index)\n",
    "\n",
    "    ## Step 3, rename the columns as cast_PC1, cast_PC2, etc. \n",
    "    col_names = []\n",
    "    for i in range(pca_n):\n",
    "            i_name = val_name + \"_PC\" + str(i+1)\n",
    "            col_names.append(i_name)\n",
    "    train_pca.columns = col_names\n",
    "    test_pca.columns = col_names\n",
    "\n",
    "    return (train_pca, test_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for Variable ['animatino department', 'original music']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Function considers a particular feature of interest (e.g. animation department, original music) \n",
    "### and counts the number of member occurance in each movie \n",
    "### Input: \n",
    "# - train_feature: df with 2500 train observations, all feature columns, indexed with imdb_ids\n",
    "# - test_feature: df with 2500 test observations, all feature columns, indexed with imdb_ids \n",
    "# - val_name # feature of interest, eg. animation department\n",
    "### Output:\n",
    "# - train_count: df with 2500 train observations, new column of count, indexed with imdb_ids\n",
    "# - test_count: df with 2500 test observations, new column of count, indexed with imdb_ids\n",
    "\n",
    "def feature_to_count(train_feature, train_genre, test_feature, test_genre,\n",
    "                val_name = 'animation department'):\n",
    "    \n",
    "    # convert column to number of counts of members \n",
    "    train_count = pd.DataFrame(string_to_vector(train_feature, val_name).ix[:,:-1].sum(axis=1), \n",
    "                               columns = {val_name + ' count'})\n",
    "    test_count = pd.DataFrame(string_to_vector(test_feature, val_name).ix[:,:-1].sum(axis=1), \n",
    "                              columns = {val_name + ' count'})\n",
    "    \n",
    "    return(train_count, test_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 2. Performance Metrics\n",
    "@Wenshan\n",
    "\n",
    "explain how do we evaluate our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 3. Benchmark Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Logistic Regression\n",
    "@Wehshan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 4. Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The notebook to submit this week should at least include:\n",
    "\n",
    "Detailed description and implementation of two different models\n",
    "\n",
    "Description of your performance metrics\n",
    "\n",
    "Careful performance evaluations for both models\n",
    "\n",
    "Visualizations of the metrics for performance evaluation\n",
    "\n",
    "Discussion of the differences between the models, their strengths, weaknesses, etc.\n",
    "\n",
    "Discussion of the performances you achieved, and how you might be able to improve them in the future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [ipykernel_py2]",
   "language": "python",
   "name": "Python [ipykernel_py2]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
