{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS109b Final Project\n",
    "\n",
    "## Milestone2\n",
    "\n",
    "by Danqing Wang, Wenshan Zheng, Zecai Liang\n",
    "\n",
    "----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### load libraries\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part1. Download Data\n",
    "\n",
    "For demenstration of code:\n",
    "use the the top 100 rows (the top 100 movies sorted by \"popularity\" in TMDB database)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Download Data from TMDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "by Wenshan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "input:\n",
    "\n",
    "n = 25k (number of data points we decide to download)\n",
    "\n",
    "output: \n",
    "\n",
    "\n",
    "data file \"imdb_id25K.txt\"\n",
    "\n",
    "data file \"data_imdb_top100.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Download Data from IMDB (DONE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "by Danqing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "input: \n",
    "\n",
    "imdb_id25K.txt\n",
    "\n",
    "\n",
    "\n",
    "output: \n",
    "\n",
    "data file \"data_imdb_top100.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def download_imdb_n(id_list = \"imdb_id25K.txt\", out_file = \"data_imdb_top100.txt\", n = 100):\n",
    "    ##### function downloads top n entires of imdb data as given by imdb_id25K.txt\n",
    "    ##### outputs downloaded data as \n",
    "\n",
    "    # Import TMDb top 25,000 movies using imdb_ids\n",
    "    ID = np.array(pd.read_csv(id_list, header=None))\n",
    "    ID = ID[:n]\n",
    "\n",
    "    # Among the different variables, we select the following variables of interest in our analysis  \n",
    "    columns = ['title','genres', 'director', 'distributors', 'year', 'rating', 'votes', 'runtimes', \n",
    "              'language codes', 'languages', 'producer', 'mpaa', 'writer', 'top 250 rank', 'kind', \n",
    "               'country codes', 'countries', 'cover url', 'aspect_ratio', 'production companies', \n",
    "               'cinematographer', 'plot outline', 'plot', 'cast', 'animation department', 'original music',\n",
    "               'canonical title', 'editorial department', 'canonical title', 'long imdb title',\n",
    "               'long imdb canonical title', 'smart canonical title', 'smart long imdb canonical title',\n",
    "               'full-size cover url']\n",
    "\n",
    "    ## Download IMDb data\n",
    "    index = range(1, len(ID)+1) \n",
    "    df = pd.DataFrame(index = index, columns=columns)\n",
    "    df = df.fillna(0)\n",
    "\n",
    "    # Fill in dataframe df \n",
    "    from imdb import IMDb\n",
    "    ia = IMDb()\n",
    "    for i in range(0, len(index)):\n",
    "        movie = ia.get_movie(ID[i]) # grab movie data by id\n",
    "        ia.update(movie)\n",
    "        keys = movie.keys() # generate the available keys of this particular movie \n",
    "        print i\n",
    "\n",
    "        for j in range(0, len(columns)):\n",
    "            if columns[j] in keys:\n",
    "                if type(movie[columns[j]]) == list:\n",
    "                    result = str(movie[columns[j]])\n",
    "                else:\n",
    "                    result = movie[columns[j]]\n",
    "                df.iloc[i,j] = result\n",
    "            else:\n",
    "                df.iloc[i,j] = 'nan'\n",
    "\n",
    "    # add column if IDs to the dataframe\n",
    "    df['imdb_ids'] = pd.Series(ID.reshape(n,), index = df.index)\n",
    "\n",
    "    # export dataframe to txt file\n",
    "    df.to_csv(out_file, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "download_imdb_n(id_list = \"imdb_id25K.txt\", out_file = \"data_imdb_top100.txt\", n = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part2. Parse and Merge Data\n",
    "\n",
    "For demenstration of code:\n",
    "use the the top 100 rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Parse Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "by Zecai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "input:\n",
    "\n",
    "data file \"data_imdb_top100.txt\"\n",
    "\n",
    "\n",
    "output:\n",
    "\n",
    "data file \"data_imdb_top100_parse.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Merge TMDB and IMDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "descripe how we merge features and genres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "by Wenshan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "input:\n",
    "\n",
    "data file \"data_tmdb_top100.txt\"\n",
    "\n",
    "data file \"data_tmdb_top100_parse.txt\"\n",
    "\n",
    "\n",
    "output: \n",
    "\n",
    "data file \"data_features_top100.txt\"\n",
    "    \n",
    "data file \"data_genres_top100.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part3. Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For demenstration of code:\n",
    "use variable `director` for the top 100 data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Variables regarding people/companies involved production (DONE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "by Danqing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#val_group1 = [\"director\", \"writer\",\"cast\",\"distributors\", \"producer\", \"production companies\",\"cinematographer\", \n",
    "#              \"animation department\", \"original music\", \"editorial department\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 2.1.1 extract important top X IDs (for each genre)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Directors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Function that takes in features csv and genres csv \n",
    "## and outputs list of top 5 directors associated with each genre\n",
    "\n",
    "def directors_by_genre(features_csv =\"feature_multi_top100.txt\", genre_csv = \"genre_multi_top100.txt\"):\n",
    "    import numpy as np \n",
    "    import pandas as pd\n",
    "    import re\n",
    "    import seaborn as sns \n",
    "    \n",
    "    features = pd.read_csv(features_csv)\n",
    "    genres = pd.read_csv(genre_csv)\n",
    "    \n",
    "    features2 = features\n",
    "    features2[\"mpaa_reason\"] = features[\"mpaa\"]\n",
    "\n",
    "    # parse director column into entries instead of str\n",
    "    for i in range(len(features)):\n",
    "        for val in [\"director\"]:\n",
    "            if type(features.ix[i,val]) == str:  # when the entry is not 'float(nan)'\n",
    "                st = features.ix[i,val]\n",
    "                value = re.findall(r'\\d+',st)\n",
    "                features2[val][i] = value\n",
    "   \n",
    "    director = features2['director']\n",
    "        \n",
    "    ##### list of directors simply by expanding out each of the 100 movie entries \n",
    "    director_list = []\n",
    "    for i in range(len(director)):\n",
    "        if director[i] > 0:\n",
    "            for j in range(len(director[i])):\n",
    "                director_list = np.append(director_list, director[i][j]) #list of directors (not unique)\n",
    "    \n",
    "    ##### dataframe of directors as index, genres as columns \n",
    "    director_genre = pd.DataFrame(index = director_list, columns = genres.columns)\n",
    "    count = 0\n",
    "\n",
    "    for i in range(len(director)):\n",
    "        if director[i] > 0:\n",
    "            for j in range(len(director[i])):\n",
    "                director_genre.ix[count,] = genres.ix[i,]\n",
    "                count = count + 1\n",
    "    \n",
    "    ##### dataframe of unique directors as index, genre as columns, counts for each genre as table content\n",
    "    director_full = pd.DataFrame(index = np.unique(director_list), columns = genres.columns)\n",
    "\n",
    "    for i in range(len(np.unique(director_list))):\n",
    "        director_full.ix[i, ] = director_genre[director_genre.index == np.unique(director_list)[i]].sum(axis = 0)\n",
    "    \n",
    "    #### Clean dataframe\n",
    "    director_full = director_full.drop('Unnamed: 0', 1) #remove first column \n",
    "    total_numbers_by_genre = director_genre.drop('Unnamed: 0', 1).sum(axis =0) #total number of each genre\n",
    "    percentage = director_full/total_numbers_by_genre #director_full normalized by genre \n",
    "    \n",
    "    #### Select the top five directors of each genre\n",
    "    genre_director=pd.DataFrame(index = percentage.columns, columns = [1,2,3,4,5])\n",
    "    for i in range(len(percentage.columns)):\n",
    "        if percentage[percentage.columns[i]].sum() > 0:\n",
    "            for j in range(0,5):\n",
    "                genre_director.iloc[i,j] = percentage.sort(columns = percentage.columns[i], ascending=False).index.values[j]\n",
    "    \n",
    "    return(genre_director)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distributors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Function that takes in features csv and genres csv \n",
    "## and outputs list of top 5 distributors associated with each genre\n",
    "\n",
    "def distributors_by_genre(features_csv =\"feature_multi_top100.txt\", genre_csv = \"genre_multi_top100.txt\"):\n",
    "    import numpy as np \n",
    "    import pandas as pd\n",
    "    import re\n",
    "    import seaborn as sns \n",
    "    \n",
    "    features = pd.read_csv(features_csv)\n",
    "    genres = pd.read_csv(genre_csv)\n",
    "    \n",
    "    features2 = features\n",
    "    features2[\"mpaa_reason\"] = features[\"mpaa\"]\n",
    "\n",
    "    # parse director column into entries instead of str\n",
    "    for i in range(len(features)):\n",
    "        for val in [\"distributors\"]:\n",
    "            if type(features.ix[i,val]) == str:  # when the entry is not 'float(nan)'\n",
    "                st = features.ix[i,val]\n",
    "                value = re.findall(r'\\d+',st)\n",
    "                features2[val][i] = value\n",
    "   \n",
    "    distributors = features2['distributors']\n",
    "        \n",
    "    ##### list of distributors simply by expanding out each of the 100 movie entries \n",
    "    distributors_list = []\n",
    "    for i in range(len(distributors)):\n",
    "        if distributors[i] > 0:\n",
    "            for j in range(len(distributors[i])):\n",
    "                distributors_list = np.append(distributors_list, distributors[i][j]) #list of distributors (not unique)\n",
    "    \n",
    "    ##### dataframe of directors as index, genres as columns \n",
    "    distributors_genre = pd.DataFrame(index = distributors_list, columns = genres.columns)\n",
    "    count = 0\n",
    "    for i in range(len(distributors)):\n",
    "        if distributors[i] > 0:\n",
    "            for j in range(len(distributors[i])):\n",
    "                distributors_genre.ix[count,] = genres.ix[i,]\n",
    "                count = count + 1\n",
    "    \n",
    "    ##### dataframe of unique directors as index, genre as columns, counts for each genre as table content\n",
    "    distributors_full = pd.DataFrame(index = np.unique(distributors_list), columns = genres.columns)\n",
    "    for i in range(len(np.unique(distributors_list))):\n",
    "        distributors_full.ix[i, ] = distributors_genre[distributors_genre.index == np.unique(distributors_list)[i]].sum(axis = 0)\n",
    "    \n",
    "    #### Clean dataframe\n",
    "    distributors_full = distributors_full.drop('Unnamed: 0', 1) #remove first column \n",
    "    total_numbers_by_genre = distributors_genre.drop('Unnamed: 0', 1).sum(axis =0) #total number of each genre\n",
    "    percentage = distributors_full/total_numbers_by_genre #director_full normalized by genre \n",
    "    \n",
    "    #### Select the top five directors of each genre\n",
    "    genre_distributors=pd.DataFrame(index = percentage.columns, columns = [1,2,3,4,5])\n",
    "    for i in range(len(percentage.columns)):\n",
    "        if percentage[percentage.columns[i]].sum() > 0:\n",
    "            for j in range(0,5):\n",
    "                genre_distributors.iloc[i,j] = percentage.sort(columns = percentage.columns[i], ascending=False).index.values[j]\n",
    "    \n",
    "    return(genre_distributors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Function that takes in features csv and genres csv \n",
    "## and outputs list of top 5 cast members associated with each genre\n",
    "\n",
    "def cast_by_genre(features_csv =\"feature_multi_top100.txt\", genre_csv = \"genre_multi_top100.txt\"):\n",
    "    import numpy as np \n",
    "    import pandas as pd\n",
    "    import re\n",
    "    import seaborn as sns \n",
    "    \n",
    "    features = pd.read_csv(features_csv)\n",
    "    genres = pd.read_csv(genre_csv)\n",
    "    \n",
    "    features2 = features\n",
    "    features2[\"mpaa_reason\"] = features[\"mpaa\"]\n",
    "\n",
    "    # parse director column into entries instead of str\n",
    "    for i in range(len(features)):\n",
    "        for val in [\"cast\"]:\n",
    "            if type(features.ix[i,val]) == str:  # when the entry is not 'float(nan)'\n",
    "                st = features.ix[i,val]\n",
    "                value = re.findall(r'\\d+',st)\n",
    "                features2[val][i] = value\n",
    "   \n",
    "    cast = features2['cast']\n",
    "        \n",
    "    ##### list of distributors simply by expanding out each of the 100 movie entries \n",
    "    cast_list = []\n",
    "    for i in range(len(cast)):\n",
    "        if cast[i] > 0:\n",
    "            for j in range(len(cast[i])):\n",
    "                cast_list = np.append(cast_list, cast[i][j]) #list of distributors (not unique)\n",
    "    \n",
    "    ##### dataframe of directors as index, genres as columns \n",
    "    cast_genre = pd.DataFrame(index = cast_list, columns = genres.columns)\n",
    "    count = 0\n",
    "    for i in range(len(cast)):\n",
    "        if cast[i] > 0:\n",
    "            for j in range(len(cast[i])):\n",
    "                cast_genre.ix[count,] = genres.ix[i,]\n",
    "                count = count + 1\n",
    "    \n",
    "    ##### dataframe of unique directors as index, genre as columns, counts for each genre as table content\n",
    "    cast_full = pd.DataFrame(index = np.unique(cast_list), columns = genres.columns)\n",
    "    for i in range(len(np.unique(cast_list))):\n",
    "        cast_full.ix[i, ] = cast_genre[cast_genre.index == np.unique(cast_list)[i]].sum(axis = 0)\n",
    "    \n",
    "    #### Clean dataframe\n",
    "    cast_full = cast_full.drop('Unnamed: 0', 1) #remove first column \n",
    "    total_numbers_by_genre = cast_genre.drop('Unnamed: 0', 1).sum(axis =0) #total number of each genre\n",
    "    percentage = cast_full/total_numbers_by_genre #director_full normalized by genre \n",
    "    \n",
    "    #### Select the top five directors of each genre\n",
    "    genre_cast=pd.DataFrame(index = percentage.columns, columns = [1,2,3,4,5])\n",
    "    for i in range(len(percentage.columns)):\n",
    "        if percentage[percentage.columns[i]].sum() > 0:\n",
    "            for j in range(0,5):\n",
    "                genre_cast.iloc[i,j] = percentage.sort(columns = percentage.columns[i], ascending=False).index.values[j]\n",
    "    \n",
    "    return(genre_cast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "directors_top5 = directors_by_genre(features_csv =\"feature_multi_top100.txt\", genre_csv = \"genre_multi_top100.txt\")\n",
    "distributors_top5 = distributors_by_genre(features_csv =\"feature_multi_top100.txt\", genre_csv = \"genre_multi_top100.txt\")\n",
    "cast_top5 = cast_by_genre(features_csv =\"feature_multi_top100.txt\", genre_csv = \"genre_multi_top100.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 2.1.2 Dimention Reduction Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "output: \n",
    "\n",
    "a data frame from column `director` that contains that first i PCs and each column is names \"director_pci\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Variables for text analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "by Zecai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# val_group2 = [\"title\",\"plot outline\", \"plot\", \"mpaa_reason\", \"tagline_TMDB\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### 2.2.1 bag-of-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### 2.2.2 dimention reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "output: \n",
    "\n",
    "a data frame from column `title` that contains that first i PCs and each column is names \"title_pci\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other discusions to include"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The exact form of X and Y depends on the ideas you had previously. In general though Y should involve the genre of a movie, and X the features you want to include to predict the genre. Remember from the lecture that more features does not necessarily equal better prediction performance. Use your application knowledge and the insight you gathered from your genre pair analysis and additional EDA to design Y. Do you want to include all genres? Are there genres that you assume to be easier to separate than others? Are there genres that could be grouped together? There is no one right answer here. We are looking for your insight, so be sure to describe your decision process in your notebook.\n",
    "\n",
    "In preparation for the deep learning part we strongly encourage you to have two sets of training data X, one with the metadata and one with the movie posters. Make sure to have a common key, like the movie ID, to be able to link the two sets together. Also be mindful of the data rate when you obtain the posters. Time your requests and choose which poster resolution you need. In most cases w500 should be sufficient, and probably a lower resolution will be fine.\n",
    "\n",
    "The notebook to submit this week should at least include:\n",
    "\n",
    "Discussion about the imbalanced nature of the data and how you want to address it\n",
    "\n",
    "Description of your data\n",
    "\n",
    "What does your choice of Y look like?\n",
    "\n",
    "Which features do you choose for X and why?\n",
    "\n",
    "How do you sample your data, how many samples, and why?\n",
    "\n",
    "Important: You do not need to upload the data itself to Canvas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
