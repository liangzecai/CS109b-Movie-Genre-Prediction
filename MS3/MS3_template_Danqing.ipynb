{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS109b Final Project\n",
    "\n",
    "# Milestone 3\n",
    "\n",
    "\n",
    "by Danqing Wang, Wenshan Zheng, Zecai Liang\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1. Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable Group 1: ['year', 'rating', 'votes', 'popularity_TMDB', 'runtime_TMDB']\n",
    "- variable ['year', 'rating', 'votes'] have missing values as 'NaN': we impute the missing value in training and test data with the mean value of training data\n",
    "- [popularity_TMDB'] have missing values as 0: we impute the missing value in training and test data with the mean value of training data\n",
    "- ['runtime_TMDB'] have missing values as 0 and as 'NaN': we perform the same mean imputation\n",
    "\n",
    "### Variables Group 2: ['title', 'plot', 'plot outline', 'overview_TMDB', 'tagline_TMDB']\n",
    "- we combine all the text information for one movie, split the paragraph of string into bag-of-words, and return the top 30 PCs\n",
    "- we used the PCA model trained from the training data to transform the test data\n",
    "\n",
    "### Variable Group 3: ['mpaa']\n",
    "- we extracted the text as the reason for 'mpaa', and applied similar text analysis as variable group 2, and return return the top 10 PCs\n",
    "\n",
    "### Variable Group 4: ['director', 'cast', 'production company', 'writer']\n",
    "- ['director', 'writer']: We analyse the training data and rank the top few most representative directors/writer in each genre. We extract the top director/writer from each genre and use them as new predictor columns, and transform the training data into a new dataframe using one-hot-encoding. We also form a similar dataframe for the test dataset using the same columns. Since the dataframes are sparse, we expect some of the directors/writers selected by the training data analysis not represented at all in the test dataset, and therefore will have columns with NULL values. We replace these NULL values with 0s. \n",
    "- ['cast', 'production companies']: We analyse the training data and rank the top 5 most representative cast members/production companies in each genre. We extract the lists of cast members/production companies and use one-hot-encoding to form a dataframe of 1s and 0s consisting of all movies in the training set. We then perform PCA on the dataframe and select the top five PCs for both cast and production companies. We then use these PCs to transform the test dataset into the new basis, and prepare it for model fitting. \n",
    "\n",
    "### Variables Group 5: ['animation department', 'original music']\n",
    "- We extract the total number of members in the 'animation department'/'original music' and form new columns of animation department count / original music count. \n",
    "\n",
    "### Variables Group 6: ['countries']\n",
    "- we return the number of countries for each movie with variable name 'country_n'\n",
    "- we also record if the movie is produced in one of the following major countries: ['usa', 'france', 'uk', 'germany', 'italy', 'canada', 'japan', 'india', 'spain'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Responses\n",
    "@ Wehshan\n",
    "- choose one between 'music' and 'musical'\n",
    "- delete 'TV-movie'\n",
    "- combined some genres (details and reason)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_feature = pd.read_csv(\"feature_final.csv\", index_col = 0)\n",
    "df_genre = pd.read_csv(\"genre_final.csv\", index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = generate_train_test_data(feature_data = df_feature, genre_data = df_genre, \n",
    "                                                             n_sample = 5000, train_ratio = 0.5, ran_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_train.to_csv(\"x_train.csv\")\n",
    "x_test.to_csv(\"x_test.csv\")\n",
    "y_train.to_csv(\"y_train.csv\")\n",
    "y_test.to_csv(\"y_test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run functions in the supplement before running this function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###### Function to generate train and test data for classification models #######\n",
    "### ------ Input ---------- ###\n",
    "# feature_data: a data frame with original features from IMDB and TMDB\n",
    "# genre_data: a data grame with genres (already merged turned into one-hot coding), \n",
    "#             matching the feature_data by 'imdb_ids'\n",
    "# n_sample: the number of data points to sample from the input data frame to fit model\n",
    "# train_ratio: the percentage of training data among sampled data\n",
    "# ran_state: seed for random sampling\n",
    "\n",
    "\n",
    "def generate_train_test_data(feature_data, genre_data, n_sample = 5000, train_ratio = 0.5, ran_state = 0):\n",
    "    \n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import re\n",
    "    import random\n",
    "    import warnings\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "    # ------------------------------------------------------------------------------------------------------ #     \n",
    "\n",
    "    ### sample data ###\n",
    "    \n",
    "    data_x = feature_data.sample(n = n_sample, random_state = ran_state)\n",
    "    data_y = genre_data.ix[data_x.index.values,]\n",
    "    \n",
    "    ### split into train and test data ###\n",
    "    k = int(n_sample * train_ratio)\n",
    "    \n",
    "    x_train = data_x[:k]\n",
    "    x_test = data_x[k:]\n",
    "    y_train = data_y[:k]\n",
    "    y_test = data_y[k:]\n",
    "    \n",
    "    \n",
    "    # ------------------------------------------------------------------------------------------------------ # \n",
    "    ### Variable Group 1: ['year', 'rating', 'votes', 'popularity_TMDB', 'runtime_TMDB'] ###\n",
    "\n",
    "    val_group1 = ['year', 'rating', 'votes', 'popularity_TMDB', 'runtime_TMDB']\n",
    "    \n",
    "    ## missing value imputation ##\n",
    "    # use the mean from train data to fill test data \n",
    "\n",
    "    # 'year', 'rating', 'votes' have missing values as 'NaN', fill by column mean\n",
    "    x_train.ix[x_train['year'].isnull(), 'year'] = x_train['year'].mean()\n",
    "    x_test.ix[x_test['year'].isnull(), 'year'] = x_train['year'].mean()\n",
    "\n",
    "    x_train.ix[x_train['rating'].isnull(), 'rating'] = x_train['rating'].mean()\n",
    "    x_test.ix[x_test['rating'].isnull(), 'rating'] = x_train['rating'].mean()\n",
    "\n",
    "    x_train.ix[x_train['votes'].isnull(), 'votes'] = x_train['votes'].mean()\n",
    "    x_test.ix[x_test['votes'].isnull(), 'votes'] = x_train['votes'].mean()\n",
    "\n",
    "    # 'popularity_TMDB' have missing values as 0, replace by column mean\n",
    "    x_train.ix[x_train['popularity_TMDB'] == 0, 'popularity_TMDB'] = x_train['popularity_TMDB'].mean()\n",
    "    x_test.ix[x_test['popularity_TMDB'] == 0, 'popularity_TMDB'] = x_train['popularity_TMDB'].mean()\n",
    "    \n",
    "    # 'runtime_TMDB' have missing values as 0 and 'NaN'\n",
    "    value = x_train['runtime_TMDB'].mean()\n",
    "    x_train.ix[x_train['runtime_TMDB'] == 0, 'runtime_TMDB'] = value\n",
    "    x_train.ix[x_train['runtime_TMDB'].isnull(), 'runtime_TMDB'] = value\n",
    "    x_test.ix[x_test['runtime_TMDB'] == 0, 'runtime_TMDB'] = value\n",
    "    x_test.ix[x_test['runtime_TMDB'].isnull(), 'runtime_TMDB'] = value\n",
    "    \n",
    "    \n",
    "    ## use x_train_new, x_test_new to record the transformed data\n",
    "    x_train_group1 = x_train[val_group1]\n",
    "    x_test_group1 = x_test[val_group1]\n",
    "\n",
    "    \n",
    "    # ------------------------------------------------------------------------------------------------------ # \n",
    "    ### Variables Group 2: ['title', 'plot', 'plot outline', 'overview_TMDB', 'tagline_TMDB', 'mpaa_reason'] ###\n",
    "\n",
    "    ## combine the text in ['title', 'plot', 'plot outline', 'overview_TMDB', 'tagline_TMDB']\n",
    "\n",
    "    x_train_text = x_train['title'].str.cat([x_train['plot'], x_train['plot outline'], \n",
    "                              x_train['overview_TMDB'], x_train['tagline_TMDB']], \n",
    "                              na_rep = \" \")\n",
    "    x_train_text = pd.DataFrame(x_train_text, index = x_train.index)\n",
    "\n",
    "    x_test_text = x_test['title'].str.cat([x_test['plot'], x_train['plot outline'], \n",
    "                              x_test['overview_TMDB'], x_test['tagline_TMDB']], \n",
    "                              na_rep = \" \")\n",
    "    x_test_text = pd.DataFrame(x_test_text, index = x_test.index)\n",
    "\n",
    "    ## apply text analysis on combined text and return the top 30 PCs\n",
    "    x_train_group2, x_test_group2 = text_analysis(x_train_text, x_test_text,\n",
    "                                             val_name = 'text', n_components = 30)\n",
    "\n",
    "    \n",
    "    # ------------------------------------------------------------------------------------------------------ # \n",
    "    ### Variables Group 3: [mpaa_reason'] ###\n",
    "\n",
    "    ## apply text analysis on 'mpaa_reason' and return thr PCs that cover 60% variance\n",
    "    x_train_mpaa = pd.DataFrame(x_train['mpaa_reason'], index = x_train.index)\n",
    "    x_test_mpaa = pd.DataFrame(x_test['mpaa_reason'], index = x_test.index)\n",
    "    \n",
    "    x_train_group3, x_test_group3 = text_analysis(x_train_mpaa, x_test_mpaa,\n",
    "                                             val_name = 'mpaa', n_components = 10)\n",
    "    \n",
    "    \n",
    "    # ------------------------------------------------------------------------------------------------------ # \n",
    "    ### Variables Group 4: ['director', 'cast', 'production company', 'writer'] ###\n",
    "    \n",
    "    x_train_group4_1, x_test_group4_1 = top_features(train_feature = x_train, train_genre = y_train,\n",
    "                                        test_feature = x_test, test_genre = y_test,\n",
    "                                        val_name = 'director',\n",
    "                                        val_n = 1)\n",
    "    \n",
    "    x_train_group4_2, x_test_group4_2 = top_features(train_feature = x_train, train_genre = y_train,\n",
    "                                        test_feature = x_test, test_genre = y_test,\n",
    "                                        val_name = 'writer',\n",
    "                                        val_n = 1)\n",
    "\n",
    "    x_train_group4_3, x_test_group4_3 = top_features_pca(train_feature = x_train, train_genre = y_train,\n",
    "                                        test_feature = x_test, test_genre = y_test,\n",
    "                                         val_name = 'cast',\n",
    "                                         val_n = 5,\n",
    "                                         pca_n = 10)\n",
    "\n",
    "    x_train_group4_4, x_test_group4_4 = top_features_pca(train_feature = x_train, train_genre = y_train,\n",
    "                                        test_feature = x_test, test_genre = y_test,\n",
    "                                         val_name = 'production companies',\n",
    "                                         val_n = 5,\n",
    "                                         pca_n = 5)\n",
    "     \n",
    "    x_train_group4 = pd.concat([x_train_group4_1, x_train_group4_2,\n",
    "                               x_train_group4_3, x_train_group4_4], axis = 1)\n",
    "    x_test_group4 = pd.concat([x_test_group4_1, x_test_group4_2,\n",
    "                               x_test_group4_3, x_test_group4_4], axis = 1)\n",
    "    \n",
    "    # ------------------------------------------------------------------------------------------------------ # \n",
    "    ### Variables Group 5: ['animation department', 'original music'] ###\n",
    "    \n",
    "    # return x_train_group5, x_test_group5\n",
    "    \n",
    "    x_train_group5_1, x_test_group5_1 = feature_to_count(train_feature = x_train, train_genre = y_train,\n",
    "                                        test_feature = x_test, test_genre = y_test,\n",
    "                                        val_name = 'animation department')\n",
    "\n",
    "    x_train_group5_2, x_test_group5_2 = feature_to_count(train_feature = x_train, train_genre = y_train,\n",
    "                                        test_feature = x_test, test_genre = y_test,\n",
    "                                        val_name = 'original music')\n",
    "    \n",
    "    \n",
    "    x_train_group5 = pd.concat([x_train_group5_1, x_train_group5_2], axis = 1)\n",
    "    x_test_group5 = pd.concat([x_test_group5_1, x_test_group5_2], axis = 1)\n",
    "    \n",
    "    \n",
    "    # ------------------------------------------------------------------------------------------------------ # \n",
    "    ### Variables Group 6: ['countries'] ###\n",
    "    x_train_country = text_to_matrix(pd.DataFrame(x_train['countries'], index = x_train.index))\n",
    "    x_test_country = text_to_matrix(pd.DataFrame(x_test['countries'], index = x_test.index))\n",
    "    \n",
    "    x_train_country.index = x_train.index\n",
    "    x_test_country.index = x_test.index\n",
    "    \n",
    "      # count the number of countries for each movie\n",
    "    x_train_group6_1 = pd.DataFrame(x_train_country.sum(axis = 1), columns = ['country_n'])\n",
    "    x_test_group6_1 = pd.DataFrame(x_test_country.sum(axis = 1), columns = ['country_n'])\n",
    "    \n",
    "      # keep the information of major countries\n",
    "    country_major = ['usa', 'france', 'uk', 'germany', 'italy', 'canada', 'japan', 'india', 'spain']\n",
    "    x_train_group6_2 = x_train_country[country_major]\n",
    "    x_train_group6_2.columns = ['country_usa', 'country_france', \n",
    "                                 'country_uk', 'country_germany',\n",
    "                                 'country_italy', 'country_canada', \n",
    "                                 'country_japan', 'country_india', 'country_spain'] \n",
    "    x_test_group6_2 = x_test_country[country_major]\n",
    "    x_test_group6_2.columns = ['country_usa', 'country_france', \n",
    "                                 'country_uk', 'country_germany',\n",
    "                                 'country_italy', 'country_canada', \n",
    "                                 'country_japan', 'country_india', 'country_spain']\n",
    "    \n",
    "    # ------------------------------------------------------------------------------------------------------ # \n",
    "    ### Combine Engineered Features ###\n",
    "    x_train_new = pd.concat([x_train_group1, x_train_group2, x_train_group3, \n",
    "                             x_train_group4, x_train_group5,\n",
    "                             x_train_group6_1, x_train_group6_2], axis=1)\n",
    "    \n",
    "    x_test_new = pd.concat([x_test_group1, x_test_group2, x_test_group3, \n",
    "                             x_test_group4, x_test_group5,\n",
    "                            x_test_group6_1, x_test_group6_2], axis=1)\n",
    "     \n",
    "    return (x_train_new, x_test_new, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supplement: functions that are used in the above meta-function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### part of the function to apply text analysis to a data series #####\n",
    "#### to transform a text paragraph to bag-of-words than to one-hot coding\n",
    "\n",
    "\n",
    "#----------------------------------------------------------------------------------------------------\n",
    "### Input ###\n",
    "        # data: a series for text analysis\n",
    "\n",
    "        \n",
    "### Output ###\n",
    "        # the transformed data in one-hot coding\n",
    "#----------------------------------------------------------------------------------------------------        \n",
    "\n",
    "\n",
    "\n",
    "def text_to_matrix(data):\n",
    "    \n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import re\n",
    "    import warnings\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    ## --------------- Bag-of-Words --------------- ##\n",
    "    \n",
    "    ## string to list\n",
    "    import re\n",
    "    col_words = []\n",
    "    \n",
    "    for i in range(data.shape[0]):\n",
    "    \n",
    "        if type(data.iloc[i,0]) == str: \n",
    "            letters_only = re.sub(\"[^a-zA-Z]\", \" \" , data.iloc[i,0]) # remove non-letter\n",
    "            lower_case = letters_only.lower().split()   # Convert to lower case # Split into words\n",
    "            \n",
    "            # avoid downloading nltk\n",
    "            # from NLTK stopwords https://pythonprogramming.net/stop-words-nltk-tutorial/\n",
    "            stops = {'ourselves', 'hers', 'between', 'yourself', 'but', 'again', 'there', 'about', 'once', \n",
    "                     'during', 'out', 'very', 'having', 'with', 'they', 'own', 'an', 'be', 'some', 'for', \n",
    "                     'do', 'its', 'yours', 'such', 'into', 'of', 'most', 'itself', 'other', 'off', 'is', \n",
    "                     's', 'am', 'or', 'who', 'as', 'from', 'him', 'each', 'the', 'themselves', 'until', \n",
    "                     'below', 'are', 'we', 'these', 'your', 'his', 'through', 'don', 'nor', 'me', 'were', \n",
    "                     'her', 'more', 'himself', 'this', 'down', 'should', 'our', 'their', 'while', 'above', \n",
    "                     'both', 'up', 'to', 'ours', 'had', 'she', 'all', 'no', 'when', 'at', 'any', 'before', \n",
    "                     'them', 'same', 'and', 'been', 'have', 'in', 'will', 'on', 'does', 'yourselves', 'then', \n",
    "                     'that', 'because', 'what', 'over', 'why', 'so', 'can', 'did', 'not', 'now', 'under', 'he', \n",
    "                     'you', 'herself', 'has', 'just', 'where', 'too', 'only', 'myself', 'which', 'those', 'i', \n",
    "                     'after', 'few', 'whom', 't', 'being', 'if', 'theirs', 'my', 'against', 'a', 'by', 'doing', \n",
    "                     'it', 'how', 'further', 'was', 'here', 'than'} \n",
    "            meaningful_words = [w for w in lower_case if not w in stops]  # Remove stop words from \"words\"\n",
    "            \n",
    "            words = ( \" \".join(meaningful_words))\n",
    "    \n",
    "        else: words = \"NA\"\n",
    "       \n",
    "        col_words.append(words)\n",
    "        \n",
    "        \n",
    "    \n",
    "    ## list to vector\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "    # Initialize the \"CountVectorizer\" object\n",
    "    vectorizer = CountVectorizer(analyzer = \"word\",   \n",
    "                                 tokenizer = None,    \n",
    "                                 preprocessor = None, \n",
    "                                 stop_words = None,   \n",
    "                                 max_features = 20000)\n",
    "\n",
    "    data_array = vectorizer.fit_transform(col_words)\n",
    "    data_array = pd.DataFrame(data_array.toarray())\n",
    "    data_array.columns = vectorizer.get_feature_names()\n",
    "    \n",
    "    return data_array\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for Variable ['title', 'plot', 'plot outline', 'overview_TMDB', 'tagline_TMDB', 'mpaa_reason']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### Function to apply text analysis to a column with `colname` in the data file `filename`\n",
    "\n",
    "\n",
    "#----------------------------------------------------------------------------------------------------\n",
    "### Input ###\n",
    "        # train: the train data in one-hot coding\n",
    "        # test: the test data in one-hot coding\n",
    "        # val_name:  variable name that's used in naming the columns as \"val_name_PCi\"\n",
    "        # n_components: if value is int, the number of PCs to return\n",
    "                       # if value between (0,1), the variance explained by the PCs returnd\n",
    "        \n",
    "### Output ###\n",
    "        # data matrix of engineered feature, one for train data and one for test data\n",
    "#----------------------------------------------------------------------------------------------------        \n",
    "\n",
    "\n",
    "def text_analysis(train, test, val_name, n_components):\n",
    "    \n",
    "    ## turn each text paragraph into one-hot coding\n",
    "    train_array = text_to_matrix(train)\n",
    "    test_array = text_to_matrix(test)\n",
    "    \n",
    "    ## take the union set of words in train and text data as column\n",
    "    ## words that don't show up are assigned 0\n",
    "    align_column = pd.concat([train_array,test_array], axis=0).fillna(0)\n",
    "    # keep only the gas-of-words from training data\n",
    "    align_column = align_column[train_array.columns]\n",
    "    \n",
    "    ## split into train and text after aligning the columns\n",
    "    train_array = align_column.iloc[0:train_array.shape[0], ]\n",
    "    test_array = align_column.iloc[train_array.shape[0]:, ]\n",
    "    \n",
    "    ## PCA\n",
    "    from sklearn.decomposition import PCA\n",
    "    pca = PCA(n_components = n_components)\n",
    "    pca.fit(train_array)\n",
    "    train_new = pd.DataFrame(pca.transform(train_array), index = train.index)\n",
    "    test_new = pd.DataFrame(pca.transform(test_array), index = test.index)\n",
    "    test_new = test_new.fillna(value = 0)\n",
    "    \n",
    "    col_names = []\n",
    "    for i in range(train_new.shape[1]):\n",
    "        i_name = val_name + \"_PC\" + str(i+1)\n",
    "        col_names.append(i_name)\n",
    "        \n",
    "    train_new.columns = col_names\n",
    "    test_new.columns = col_names\n",
    "    \n",
    "    return (train_new, test_new)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function used in the following functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Function to convert to string list [u'Action', u'Adventure', u'Fantasy'] into dummy coding\n",
    "## input: \n",
    "           # data = orignal data frame, \n",
    "           # val_name = name of the variable\n",
    "## output: a data frame\n",
    "\n",
    "def string_to_vector(data, val_name):\n",
    "    \n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    \n",
    "    # convert any np.nan to a string 'nan'\n",
    "    data[val_name][pd.isnull(data[val_name])] = 'nan'\n",
    "    \n",
    "    vectorizer = CountVectorizer(analyzer = \"word\",   \n",
    "                                             tokenizer = None,    \n",
    "                                             preprocessor = None, \n",
    "                                             stop_words = None,   \n",
    "                                             max_features = 50000)\n",
    "\n",
    "    val_data = vectorizer.fit_transform(data[val_name])\n",
    "    df_val = pd.DataFrame(val_data.toarray())\n",
    "    df_val.columns = vectorizer.get_feature_names()\n",
    "    df_val.index = data.index\n",
    "    \n",
    "    return df_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for Variable ['director', 'writer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Function considers a particular feature of interest (e.g. director, writer) \n",
    "### and picks out the top val_n most important value in each genre\n",
    "### Input: \n",
    "# - train_feature: df with 2500 train observations, all feature columns, indexed with imdb_ids\n",
    "# - train_genre: df with 2500 train observations, all genre columns, indexed with imdb_ids\n",
    "# - test_feature: df with 2500 test observations, all feature columns, indexed with imdb_ids \n",
    "# - test_genre: df with 2500 test observations, all genre columns, indexed with imdb_ids\n",
    "# - val_name # feature of interest, eg. director\n",
    "# - val_n # number of top values to take, eg. 1\n",
    "### Output:\n",
    "# - train_val: df with 2500 train observations, new columns of top directors, indexed with imdb_ids\n",
    "# - test_val: df with 2500 test observations, new columns of top directors, indexed with imdb_ids\n",
    "\n",
    "def top_features(train_feature, train_genre, test_feature, test_genre,\n",
    "                val_name = 'director', val_n = 1):\n",
    "\n",
    "    # convert feature of interest into dummy variables in train set and test set\n",
    "    feature_val_train = string_to_vector(train_feature, val_name)\n",
    "    feature_val_test = string_to_vector(test_feature, val_name)\n",
    "\n",
    "    # create a dataframe with columns consisting of all directors and all genres, rows are movie entries \n",
    "    feature_val_genre = pd.concat([feature_val_train, train_genre], axis = 1)\n",
    "    \n",
    "    # generate list of top directors in each genre \n",
    "    val_list = []\n",
    "    for i in train_genre.columns:\n",
    "        sum_val_in_genre = feature_val_genre.ix[feature_val_genre[i] == 1, range(feature_val_train.shape[1]-1)].sum(axis = 0)\n",
    "        sum_val_in_genre_sorted = sum_val_in_genre.sort(inplace=False, ascending = False)\n",
    "        for j in range(val_n):\n",
    "            top_val_in_genre = sum_val_in_genre_sorted.index[j]\n",
    "            val_list.append(top_val_in_genre)      \n",
    "\n",
    "    # output dataframes of movies with new columns\n",
    "    train_val = feature_val_train.ix[:, val_list] \n",
    "    test_val = feature_val_test.ix[:, val_list]\n",
    "    \n",
    "    # replace any NA vaalues in the test set with 0\n",
    "    test_val = test_val.fillna(value = 0)\n",
    "    \n",
    "    # rename columns as director1, director2, etc\n",
    "    col_names = []\n",
    "    for i in range(train_val.shape[1]):\n",
    "            i_name = val_name + str(i)\n",
    "            col_names.append(i_name)\n",
    "    train_val.columns = col_names\n",
    "    test_val.columns = col_names\n",
    "\n",
    "    return(train_val, test_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for Variable ['cast','production company']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Function considers a particular feature of interest (e.g. cast, production companies) \n",
    "### and picks out the top val_n most important value in each genre\n",
    "### and then performs PCA, picks out the top pca_n numbers of components\n",
    "### Input: \n",
    "# - train_feature: df with 2500 train observations, all feature columns, indexed with imdb_ids\n",
    "# - train_genre: df with 2500 train observations, all genre columns, indexed with imdb_ids\n",
    "# - test_feature: df with 2500 test observations, all feature columns, indexed with imdb_ids \n",
    "# - test_genre: df with 2500 test observations, all genre columns, indexed with imdb_ids\n",
    "# - val_name # feature of interest, eg. director\n",
    "# - val_n # number of top values to take, eg. 5\n",
    "# - pca_n # number of pca components to retain, eg. 5\n",
    "### Output:\n",
    "# - train_val: df with 3000 train observations, columns as pca components, indexed with imdb_ids\n",
    "# - test_val: df with 2000 test observations, columns as pca components, indexed with imdb_ids\n",
    "\n",
    "def top_features_pca(train_feature, train_genre, test_feature, test_genre,\n",
    "                     val_name = 'cast',\n",
    "                     val_n = 5,\n",
    "                     pca_n = 5):\n",
    "    \n",
    "    ## Step 1, pick top casts in each genre using top_feature function \n",
    "    train_val, test_val = top_features(train_feature = train_feature,\n",
    "                    train_genre = train_genre,\n",
    "                    test_feature = test_feature,\n",
    "                    test_genre = test_genre,\n",
    "                    val_name = val_name,\n",
    "                    val_n = val_n)\n",
    "    \n",
    "    # replace any NA values in test set with 0 (or will have problem in pca.transform)\n",
    "    test_val = test_val.fillna(value=0)\n",
    "\n",
    "    ## Step 2, perform PCA\n",
    "    from sklearn.decomposition import PCA\n",
    "    \n",
    "    pca = PCA(n_components = pca_n, svd_solver = \"full\") # keep the first pca_n PCs\n",
    "    pca = pca.fit(train_val)\n",
    "    train_pca = pd.DataFrame(pca.transform(train_val), index = train_val.index)\n",
    "    test_pca = pd.DataFrame(pca.transform(test_val), index = test_val.index)\n",
    "\n",
    "    ## Step 3, rename the columns as cast_PC1, cast_PC2, etc. \n",
    "    col_names = []\n",
    "    for i in range(pca_n):\n",
    "            i_name = val_name + \"_PC\" + str(i+1)\n",
    "            col_names.append(i_name)\n",
    "    train_pca.columns = col_names\n",
    "    test_pca.columns = col_names\n",
    "\n",
    "    return (train_pca, test_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for Variable ['animatino department', 'original music']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Function considers a particular feature of interest (e.g. animation department, original music) \n",
    "### and counts the number of member occurance in each movie \n",
    "### Input: \n",
    "# - train_feature: df with 2500 train observations, all feature columns, indexed with imdb_ids\n",
    "# - test_feature: df with 2500 test observations, all feature columns, indexed with imdb_ids \n",
    "# - val_name # feature of interest, eg. animation department\n",
    "### Output:\n",
    "# - train_count: df with 2500 train observations, new column of count, indexed with imdb_ids\n",
    "# - test_count: df with 2500 test observations, new column of count, indexed with imdb_ids\n",
    "\n",
    "def feature_to_count(train_feature, train_genre, test_feature, test_genre,\n",
    "                val_name = 'animation department'):\n",
    "    \n",
    "    # convert column to number of counts of members \n",
    "    train_count = pd.DataFrame(string_to_vector(train_feature, val_name).ix[:,:-1].sum(axis=1), \n",
    "                               columns = {val_name + ' count'})\n",
    "    test_count = pd.DataFrame(string_to_vector(test_feature, val_name).ix[:,:-1].sum(axis=1), \n",
    "                              columns = {val_name + ' count'})\n",
    "    \n",
    "    return(train_count, test_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 2. Performance Metrics\n",
    "@Wenshan\n",
    "\n",
    "explain how do we evaluate our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to calculate the accuracy score of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function f1_genre\n",
    "# input: two pandas dataframes, \n",
    "    # genre_real: predicted values\n",
    "    # genre_predict: real values\n",
    "# output: mean f1 score of each class\n",
    "def f1_genres(genre_real, genre_predict):\n",
    "    count_row = len(genre_real)\n",
    "    if count_row == 0:\n",
    "        print \"No data in dataframe!\"\n",
    "        return\n",
    "    if count_row != len(genre_predict):\n",
    "        print \"Different length of predicted and real dataframes!\"\n",
    "        return\n",
    "    count_col = len(genre_real.columns)\n",
    "    if count_col == 0:\n",
    "        print \"No data in dataframe!\"\n",
    "        return\n",
    "    if count_col != len(genre_predict.columns):\n",
    "        print \"Different genres of predicted and real dataframes!\"\n",
    "        return\n",
    "    score = 0\n",
    "    for i in range(count_col):\n",
    "        score += f1_score(genre_real[genre_real.columns.values[i]], genre_predict[genre_predict.columns.values[i]])\n",
    "    score = score/count_col\n",
    "    return(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 3. Benchmark Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Logistic Regression\n",
    "@Wehshan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 4. Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The notebook to submit this week should at least include:\n",
    "\n",
    "Detailed description and implementation of two different models\n",
    "\n",
    "Description of your performance metrics\n",
    "\n",
    "Careful performance evaluations for both models\n",
    "\n",
    "Visualizations of the metrics for performance evaluation\n",
    "\n",
    "Discussion of the differences between the models, their strengths, weaknesses, etc.\n",
    "\n",
    "Discussion of the performances you achieved, and how you might be able to improve them in the future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
